geom_smooth(aes(x=x,y=y, colour=type), method = "lm", formula = y ~ splines::bs(x, 3), se = FALSE)+
guides(alpha=FALSE)+
xlab("Proportion of adherent participants in each arm")+
ylab("Treatment effect")+
theme_minimal()+
scale_colour_manual(values=cbPalette.multi)+
theme(legend.title=element_blank(), legend.text=element_text(size=legendfontsize), legend.position="none")+
scale_x_continuous(limits=c(start.interval, 1))+
scale_y_continuous(limits=c(ymin, ymax))+
geom_hline(yintercept=true.effect, linetype='dashed', color='red', size=0.5)
bias.plot = ggarrange(itt, pp, mpp, iv,
ncol = 2, nrow = 2,
legend = "none")
return(bias.plot)
}
rm(list=ls())
source(file = 'setup_code100220.R')
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n=n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect=true.effect, ymin=-0.2, ymax=0.3)
rm(list=ls())
source(file = 'setup_code100220.R')
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding == 'nonconfounding', bias == T)
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding == 'nonconfounding', bias == T, nonadhere.pop = nonadhere.pop)
################################################################################################################
###################Using causal inference to address non-adherence in non inferiority trials####################
#############################################Analysis functions#################################################
################################################################################################################
analysis.estimate <- function(simdata){
## intention to treat
pz1.value = mean(simdata[which(simdata[,2]==1),][,5])
pz0.value = mean(simdata[which(simdata[,2]==0),][,5])
eff.itt = pz1.value-pz0.value
## per protocol
pp = simdata[which(simdata[,2]==simdata[,4]),] # per protocol population
p.experiment.vector = pp[which(pp[,2]==1), ][,5]
p.experiment.value = mean(p.experiment.vector)
p.stdcare.vector = pp[which(pp[,2]==0),][,5]
p.stdcare.value = mean(p.stdcare.vector)
eff.pp = p.experiment.value-p.stdcare.value
## inverse probability weights on per protocol patients
pp = as.data.frame(pp)
colnames(pp) = c('id','randomisation','confounder','intervention','outcome')
ipwmodel = glm(intervention~confounder,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score = predict(ipwmodel, type="response")
weight = pp$intervention*mean(pp$intervention)/score+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel = glm(outcome~intervention, family=binomial(link="identity"), weights=weight, data=pp) #identity link for risk difference
eff.mpp = coef(outcomemodel)[2]
# iv with 2 stage regression
asmm = gmm(simdata[,5] ~ simdata[,4], x=simdata[,2], vcov="iid")
eff.iv = summary(asmm)$coefficients[2,1]
return(c(eff.iv, eff.itt, eff.mpp, eff.pp))
}
analysis.unknown <- function(simdata){
## intention to treat
pz1.value = mean(simdata[which(simdata[,2]==1),][,7])
pz0.value = mean(simdata[which(simdata[,2]==0),][,7])
eff.itt = pz1.value-pz0.value
## per protocol
pp = simdata[which(simdata[,2]==simdata[,6]),] # perprotocol population
p.experiment.vector = pp[which(pp[,2]==1),][,7]
p.experiment.value = mean(p.experiment.vector)
p.stdcare.vector = pp[which(pp[,2]==0),][,7]
p.stdcare.value = mean(p.stdcare.vector)
eff.pp = p.experiment.value-p.stdcare.value
## inverse probability weights on per protocol patients
pp=as.data.frame(pp)
colnames(pp)=c('id','randomisation','confounder', 'known', 'unknown1','unknown2','unknown3','intervention','outcome')
ipwmodel.known=glm(intervention~known,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.known=predict(ipwmodel.known, type="response")
weight.known= pp$intervention*mean(pp$intervention)/score.known+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.known)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.known=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.known, data=pp) #identity link for risk difference
eff.mpp.known=coef(outcomemodel.known)[2]
ipwmodel.unknown1=glm(intervention~known+unknown1,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.unknown1=predict(ipwmodel.unknown1, type="response")
weight.unknown1= pp$intervention*mean(pp$intervention)/score.unknown1+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.unknown1)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.unknown1=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.unknown1, data=pp) #identity link for risk difference
eff.mpp.unknown1=coef(outcomemodel.unknown1)[2]
ipwmodel.unknown2=glm(intervention~known+unknown1+unknown2,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.unknown2=predict(ipwmodel.unknown2, type="response")
weight.unknown2= pp$intervention*mean(pp$intervention)/score.unknown2+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.unknown2)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.unknown2=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.unknown2, data=pp) #identity link for risk difference
eff.mpp.unknown2=coef(outcomemodel.unknown2)[2]
ipwmodel.all=glm(intervention~confounder,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.all=predict(ipwmodel.all, type="response")
weight.all= pp$intervention*mean(pp$intervention)/score.all+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.all)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.all=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.all, data=pp) #identity link for risk difference
eff.mpp.all=coef(outcomemodel.all)[2]
# iv with 2 stage regression
asmm  =  gmm(simdata[,7] ~ simdata[,6], x=simdata[,2], vcov="iid")
eff.iv = summary(asmm)$coefficients [2,1]
return(c(eff.iv, eff.itt, eff.mpp, eff.pp))
}
sim.analysis <- function(nonconfounding, bias, nonadhere.pop){
#make up vectors for simulations
.estimate = c() #output from each simulation
estimate = c()  #for saving output from each interval
#non-adherent population
if (nonadhere.pop=="both") {
adhere.experiment = interval
adhere.stdcare = interval
} else if (nonadhere.pop=="experimental") {
adhere.experiment =  interval
adhere.stdcare =  rep(1, length(interval))
} else { #nonadhere.pop=="stdcare"
adhere.experiment = rep(1, length(interval))
adhere.stdcare = interval
}
id = 1 : (2*n) #create participant id
#simulate and derive treatment effect
for(i in 1:length(interval)) { print(paste("interval value", i,"out of",length(interval)))
for(l in 1:nIterations) {
tryCatch({
if (nonconfounding == 'nonconfounding') { #simulate data
simdata = simdata.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, nonadhere.pop = nonadhere.pop, cross.over = cross.over, i=i)
} else  if (nonconfounding == 'confounding' ) {
simdata = simdata.confounding(n = n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention, i=i)
} else {
simdata = simdata.unknownconfounding(n = n, p.experiment=p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, i=i, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention)
}
.estimate[[l]] = analysis.estimate(simdata = simdata) #gives a vector of estimates in the order of iv, itt, ipw, pp for each iteration
}, error=function(e){cat("ERROR :", conditionMessage(e), "\n")}) #receive error message if there is an error
}
#save results of every interval - each column refers to each analysis method
estimate[[i]] = matrix(unlist(.estimate), ncol = length(analysis.method), byrow = TRUE)
}
if (bias == T) { #return the estimate matrix
return (estimate)
} else { #go on to get upper bounds for type 1 error
#get sds for each interval
sd.vector = lapply(estimate, function(x) {unlist(apply(x, 2, sd))})
#get upper bounds of 95%CI for each iteration of every interval
ub.df = c()
for (i in 1:length(interval)) {
ub.df[[i]] = estimate[[i]] + qnorm(0.975) * rep(sd.vector[[i]], nIterations)
}
#get critical values for each interval
z.vector = lapply(ub.df, function(x) {unlist(quantile(x, probs = 0.025))})
#get type 1 error for each interval
t1 = c()
for (i in 1:length(interval)) {
t1[[i]] = colMeans(ub.df[[i]] < z.vector[[i]])
}
t1.df = as.data.frame(matrix(unlist(t1), ncol=4, byrow=TRUE))
return (t1.df)
}
}
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding == 'nonconfounding', bias == T, nonadhere.pop = nonadhere.pop)
#make up vectors for simulations
.estimate = c() #output from each simulation
estimate = c()  #for saving output from each interval
#non-adherent population
if (nonadhere.pop=="both") {
adhere.experiment = interval
adhere.stdcare = interval
} else if (nonadhere.pop=="experimental") {
adhere.experiment =  interval
adhere.stdcare =  rep(1, length(interval))
} else { #nonadhere.pop=="stdcare"
adhere.experiment = rep(1, length(interval))
adhere.stdcare = interval
}
nonadhere.pop = "both"
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding == 'nonconfounding', bias == T, nonadhere.pop = nonadhere.pop)
#make up vectors for simulations
.estimate = c() #output from each simulation
estimate = c()  #for saving output from each interval
#non-adherent population
if (nonadhere.pop=="both") {
adhere.experiment = interval
adhere.stdcare = interval
} else if (nonadhere.pop=="experimental") {
adhere.experiment =  interval
adhere.stdcare =  rep(1, length(interval))
} else { #nonadhere.pop=="stdcare"
adhere.experiment = rep(1, length(interval))
adhere.stdcare = interval
}
id = 1 : (2*n) #create participant id
#simulate and derive treatment effect
for(i in 1:length(interval)) { print(paste("interval value", i,"out of",length(interval)))
for(l in 1:nIterations) {
tryCatch({
if (nonconfounding == 'nonconfounding') { #simulate data
simdata = simdata.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, nonadhere.pop = nonadhere.pop, cross.over = cross.over, i=i)
} else  if (nonconfounding == 'confounding' ) {
simdata = simdata.confounding(n = n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention, i=i)
} else {
simdata = simdata.unknownconfounding(n = n, p.experiment=p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, i=i, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention)
}
.estimate[[l]] = analysis.estimate(simdata = simdata) #gives a vector of estimates in the order of iv, itt, ipw, pp for each iteration
}, error=function(e){cat("ERROR :", conditionMessage(e), "\n")}) #receive error message if there is an error
}
#save results of every interval - each column refers to each analysis method
estimate[[i]] = matrix(unlist(.estimate), ncol = length(analysis.method), byrow = TRUE)
}
rm(list=ls())
source(file = 'setup_code100220.R')
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n=n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect=true.effect, ymin=-0.2, ymax=0.3)
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding = 'nonconfounding', bias = T, nonadhere.pop = nonadhere.pop)
################################################################################################################
###################Using causal inference to address non-adherence in non inferiority trials####################
#############################################Analysis functions#################################################
################################################################################################################
analysis.estimate <- function(simdata){
## intention to treat
pz1.value = mean(simdata[which(simdata[,2]==1),][,5])
pz0.value = mean(simdata[which(simdata[,2]==0),][,5])
eff.itt = pz1.value-pz0.value
## per protocol
pp = simdata[which(simdata[,2]==simdata[,4]),] # per protocol population
p.experiment.vector = pp[which(pp[,2]==1), ][,5]
p.experiment.value = mean(p.experiment.vector)
p.stdcare.vector = pp[which(pp[,2]==0),][,5]
p.stdcare.value = mean(p.stdcare.vector)
eff.pp = p.experiment.value-p.stdcare.value
## inverse probability weights on per protocol patients
pp = as.data.frame(pp)
colnames(pp) = c('id','randomisation','confounder','intervention','outcome')
ipwmodel = glm(intervention~confounder,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score = predict(ipwmodel, type="response")
weight = pp$intervention*mean(pp$intervention)/score+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel = glm(outcome~intervention, family=binomial(link="identity"), weights=weight, data=pp) #identity link for risk difference
eff.mpp = coef(outcomemodel)[2]
# iv with 2 stage regression
asmm = gmm(simdata[,5] ~ simdata[,4], x=simdata[,2], vcov="iid")
eff.iv = summary(asmm)$coefficients[2,1]
return(c(eff.iv, eff.itt, eff.mpp, eff.pp))
}
analysis.unknown <- function(simdata){
## intention to treat
pz1.value = mean(simdata[which(simdata[,2]==1),][,7])
pz0.value = mean(simdata[which(simdata[,2]==0),][,7])
eff.itt = pz1.value-pz0.value
## per protocol
pp = simdata[which(simdata[,2]==simdata[,6]),] # perprotocol population
p.experiment.vector = pp[which(pp[,2]==1),][,7]
p.experiment.value = mean(p.experiment.vector)
p.stdcare.vector = pp[which(pp[,2]==0),][,7]
p.stdcare.value = mean(p.stdcare.vector)
eff.pp = p.experiment.value-p.stdcare.value
## inverse probability weights on per protocol patients
pp=as.data.frame(pp)
colnames(pp)=c('id','randomisation','confounder', 'known', 'unknown1','unknown2','unknown3','intervention','outcome')
ipwmodel.known=glm(intervention~known,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.known=predict(ipwmodel.known, type="response")
weight.known= pp$intervention*mean(pp$intervention)/score.known+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.known)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.known=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.known, data=pp) #identity link for risk difference
eff.mpp.known=coef(outcomemodel.known)[2]
ipwmodel.unknown1=glm(intervention~known+unknown1,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.unknown1=predict(ipwmodel.unknown1, type="response")
weight.unknown1= pp$intervention*mean(pp$intervention)/score.unknown1+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.unknown1)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.unknown1=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.unknown1, data=pp) #identity link for risk difference
eff.mpp.unknown1=coef(outcomemodel.unknown1)[2]
ipwmodel.unknown2=glm(intervention~known+unknown1+unknown2,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.unknown2=predict(ipwmodel.unknown2, type="response")
weight.unknown2= pp$intervention*mean(pp$intervention)/score.unknown2+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.unknown2)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.unknown2=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.unknown2, data=pp) #identity link for risk difference
eff.mpp.unknown2=coef(outcomemodel.unknown2)[2]
ipwmodel.all=glm(intervention~confounder,family=binomial(link="logit"), data=pp) #calculate denominators used in inverse probability weights
score.all=predict(ipwmodel.all, type="response")
weight.all= pp$intervention*mean(pp$intervention)/score.all+(1-pp$intervention)*(1-mean(pp$intervention))/(1-score.all)#create stabilized weights, using a null model with intervention as the dependent variable
outcomemodel.all=glm(outcome~intervention, family=binomial(link="identity"), weights=weight.all, data=pp) #identity link for risk difference
eff.mpp.all=coef(outcomemodel.all)[2]
# iv with 2 stage regression
asmm  =  gmm(simdata[,7] ~ simdata[,6], x=simdata[,2], vcov="iid")
eff.iv = summary(asmm)$coefficients [2,1]
return(c(eff.iv, eff.itt, eff.mpp, eff.pp))
}
sim.analysis <- function(nonconfounding, bias){
#make up vectors for simulations
.estimate = c() #output from each simulation
estimate = c()  #for saving output from each interval
#non-adherent population
if (nonadhere.pop=="both") {
adhere.experiment = interval
adhere.stdcare = interval
} else if (nonadhere.pop=="experimental") {
adhere.experiment =  interval
adhere.stdcare =  rep(1, length(interval))
} else { #nonadhere.pop=="stdcare"
adhere.experiment = rep(1, length(interval))
adhere.stdcare = interval
}
id = 1 : (2*n) #create participant id
#simulate and derive treatment effect
for(i in 1:length(interval)) { print(paste("interval value", i,"out of",length(interval)))
for(l in 1:nIterations) {
tryCatch({
if (nonconfounding == 'nonconfounding') { #simulate data
simdata = simdata.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, nonadhere.pop = nonadhere.pop, cross.over = cross.over, i=i)
} else  if (nonconfounding == 'confounding' ) {
simdata = simdata.confounding(n = n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention, i=i)
} else {
simdata = simdata.unknownconfounding(n = n, p.experiment=p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, i=i, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention)
}
.estimate[[l]] = analysis.estimate(simdata = simdata) #gives a vector of estimates in the order of iv, itt, ipw, pp for each iteration
}, error=function(e){cat("ERROR :", conditionMessage(e), "\n")}) #receive error message if there is an error
}
#save results of every interval - each column refers to each analysis method
estimate[[i]] = matrix(unlist(.estimate), ncol = length(analysis.method), byrow = TRUE)
}
if (bias == T) { #return the estimate matrix
return (estimate)
} else { #go on to get upper bounds for type 1 error
#get sds for each interval
sd.vector = lapply(estimate, function(x) {unlist(apply(x, 2, sd))})
#get upper bounds of 95%CI for each iteration of every interval
ub.df = c()
for (i in 1:length(interval)) {
ub.df[[i]] = estimate[[i]] + qnorm(0.975) * rep(sd.vector[[i]], nIterations)
}
#get critical values for each interval
z.vector = lapply(ub.df, function(x) {unlist(quantile(x, probs = 0.025))})
#get type 1 error for each interval
t1 = c()
for (i in 1:length(interval)) {
t1[[i]] = colMeans(ub.df[[i]] < z.vector[[i]])
}
t1.df = as.data.frame(matrix(unlist(t1), ncol=4, byrow=TRUE))
return (t1.df)
}
}
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding = 'nonconfounding', bias = T)
rm(list=ls())
source(file = 'setup_code100220.R')
######parameters######
#assuming that the experimental and control treatments have the same treatment efficacy of 60%
#                  non-inferiority margin of 10%
#                  type 1 error of 0.025
#                  90% power
#This required with 505 participants per arm
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n=n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect=true.effect, ymin=-0.2, ymax=0.3)
rm(list=ls())
source(file = 'setup_code100220.R')
######parameters######
#assuming that the experimental and control treatments have the same treatment efficacy of 60%
#                  non-inferiority margin of 10%
#                  type 1 error of 0.025
#                  90% power
#This required with 505 participants per arm
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
bias.nonconfounding
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect = true.effect, ymin = -0.2, ymax = 0.3)
adhere.experiment
interval
rm(list=ls())
source(file = 'setup_code100220.R')
######parameters######
#assuming that the experimental and control treatments have the same treatment efficacy of 60%
#                  non-inferiority margin of 10%
#                  type 1 error of 0.025
#                  90% power
#This required with 505 participants per arm
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect = true.effect, ymin = -0.2, ymax = 0.3)
rm(list=ls())
source(file = 'setup_code100220.R')
######parameters######
#assuming that the experimental and control treatments have the same treatment efficacy of 60%
#                  non-inferiority margin of 10%
#                  type 1 error of 0.025
#                  90% power
#This required with 505 participants per arm
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect = true.effect, ymin = -0.2, ymax = 0.3)
rm(list=ls())
source(file = 'setup_code100220.R')
######parameters######
#assuming that the experimental and control treatments have the same treatment efficacy of 60%
#                  non-inferiority margin of 10%
#                  type 1 error of 0.025
#                  90% power
#This required with 505 participants per arm
n = 505
p.experiment= 0.4
p.stdcare = 0.3
p.alt = 0.5
true.effect = p.experiment-p.stdcare
nIterations = 20
NImargin = 0.1
width = 15
height = 10
legendfontsize=12
#Run simulations for case 1 (both groups non adherent, non confounding)
b1both.cross<-bias.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = T, nIterations=nIterations, interval = interval, nonadhere.pop = "both", true.effect = true.effect, ymin = -0.2, ymax = 0.3)
#get estimates for each interval over nIterations
estimate.df = sim.analysis(nonconfounding = 'nonconfounding', bias = T, nonadhere.pop = nonadhere.pop, interval = interval)
#make up vectors for simulations
.estimate = c() #output from each simulation
estimate = c()  #for saving output from each interval
#non-adherent population
if (nonadhere.pop == "both") {
adhere.experiment = interval
adhere.stdcare = interval
} else if (nonadhere.pop == "experimental") {
adhere.experiment =  interval
adhere.stdcare =  rep(1, length(interval))
} else { #nonadhere.pop=="stdcare"
adhere.experiment = rep(1, length(interval))
adhere.stdcare = interval
}
nonadhere.pop = 'both'
interval
sim.analysis(nonconfounding = 'nonconfounding', bias = T, nonadhere.pop = nonadhere.pop, interval = interval)
#non-adherent population
if (nonadhere.pop == "both") {
adhere.experiment = interval
adhere.stdcare = interval
} else if (nonadhere.pop == "experimental") {
adhere.experiment =  interval
adhere.stdcare =  rep(1, length(interval))
} else { #nonadhere.pop=="stdcare"
adhere.experiment = rep(1, length(interval))
adhere.stdcare = interval
}
id = 1 : (2*n) #create participant id
#simulate and derive treatment effect
for(i in 1:length(interval)) { print(paste("interval value", i,"out of",length(interval)))
for(l in 1:nIterations) {
tryCatch({
if (nonconfounding == 'nonconfounding') { #simulate data
simdata = simdata.nonconfounding(n = n, p.experiment = p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, nonadhere.pop = nonadhere.pop, cross.over = cross.over, i=i)
} else  if (nonconfounding == 'confounding' ) {
simdata = simdata.confounding(n = n, p.experiment=p.experiment, p.stdcare=p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention, i=i)
} else {
simdata = simdata.unknownconfounding(n = n, p.experiment=p.experiment, p.stdcare = p.stdcare, p.alt = p.alt, cross.over = cross.over, nonadhere.pop = nonadhere.pop, i=i, confounder.outcome = confounder.outcome, confounder.intervention = confounder.intervention)
}
.estimate[[l]] = analysis.estimate(simdata = simdata) #gives a vector of estimates in the order of iv, itt, ipw, pp for each iteration
}, error=function(e){cat("ERROR :", conditionMessage(e), "\n")}) #receive error message if there is an error
}
#save results of every interval - each column refers to each analysis method
estimate[[i]] = matrix(unlist(.estimate), ncol = length(analysis.method), byrow = TRUE)
}
